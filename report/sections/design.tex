In this section we first introduce the sequential version of the Jacobi iterative algorithm, then we explain how to adapt the algorithm for vectorization and parallelization.
We also provide an performance model, to justify parallelization and design choices.

\subsection{Sequential algorithm and performance}\label{subsec:seq}
In the numerical computing literature the Jacobi iterative method is presented as particular iterative method and the corresponding pseudo-code is usually similar to the one in Algorithm~\ref{alg:sequantialjacobi}.

Supposing that a program implementing the algorithm takes $T_{jacobi}(n)$ time to compute the result, then the completion time $T_C$ of an hypothetical program is
\[
	T_C \approx T_{alloc}(n) + T_{fill}(n) + T_{jacobi}(n)
\]

where $T_{alloc}(n)$ and $T_{fill}(n)$ are, respectively, the time needed do allocate and fill the memory to store the input data.

Further expanding $T_{jacobi}$ and assuming an implementation of Algorithm~\ref{alg:sequantialjacobi} we get:
\[
	T_{jacobi}(n) \approx k \cdot (T_{conv}(n) + T_{comp}(n) + T_{upd}(n))
\]
where $k$ is the number of iterations, $T_{conv}(n)$ is the time needed to check convergence, $T_{comp}(n)$ is the time needed to compute the new approximation of the solution, and $T_{upd}(n)$ is the time needed to update the solution vector.

\begin{algorithm}[ht]
	\vspace{1em}
	\KwData{A linear system in the form of $Ax = b$ with $N \times N$ the size of $A$, a maximum accepted error $\varepsilon$, and a maximum number of iterations $K$.}
	\KwResult{An approximated solution $x^{(k)}$ s.t. $||b - Ax^{(k)}||^{2}_2 \leq \varepsilon$ or $k \ge K$.}
	\BlankLine			
	$x^{(0)} \leftarrow $ initial guess for the solution\\
	$k \leftarrow 0$\\
	\While {$||b - Ax^{(k)}||^{2}_2 \ge \varepsilon$ and $k \leq K$}
	{
		\For{$i\leftarrow 0$ \KwTo $N$}{
			$\sigma \leftarrow 0$\\
			\For{$j\leftarrow 0$ \KwTo $N$}{
				\If{$j \neq i$}{
					$\sigma$ $\leftarrow$ $\sigma + a_{ij}x^{(k)}_j$
				}
			}
			$x^{(k+1)}_i \leftarrow \frac{b_i - \sigma}{a_{ii}}$\\
		}
		$k \leftarrow k + 1$\\
	}
\caption{Pseudo-code for the sequential Jacobi iterative method.}
\label{alg:sequantialjacobi}
\end{algorithm}


\subsection{Parallel algorithm and performance}\label{subsec:par}
Given the performance model of the previous sub-section to better parallelize the algorithm we may consider to minimize $T_{conv}(n)$, $T_{comp}(n)$ , and $T_{upd}(n)$ but:
\begin{itemize}
	\item $T_{conv}(n)$ and $T_{upd}(n)$ correspond to computations with linear time complexity
	\item $ T_{comp}(n)$ corresponds to a computation with quadratic time complexity
\end{itemize}
Hence we will consider $T_{conv}(n)$ and $T_{upd}(n)$ as negligible, and proceed by parallelizing only of the last computation (always assuring that vectorization of each of the three computations takes place, see Section~\ref{sec:implementation}).

More specifically given the system matrix $A$ with size $N\times N$ and having $w$ workers:
\begin{enumerate}
	\item First, an emitter should divide $A$ into sub-matrices with $\approx N/w$ rows each, and deliver each of them to a worker,
	\item then, each worker should compute its part of the solution using the provided sub-matrix as described in the pseudo-code of Algorithm~\ref{alg:paralgo},
	\item finally a collector should recompose the solution.
\end{enumerate}

\begin{algorithm}[ht]
\vspace{1em}
\KwData{A sub-matrix $A_s$ of $A$ with $M$ rows and $N$ columns, the previous approximation $x^{(k)}$.}
\KwResult{The next approximation of the solution.}
\BlankLine
\For{$i\leftarrow 0$ \KwTo $M$}{
	$\sigma \leftarrow 0$\\
	\For{$j\leftarrow 0$ \KwTo $N$}{
		\If{$j \neq i$}{
			$\sigma$ $\leftarrow$ $\sigma + a_{ij}x^{(k)}_j$
		}
	}
	$x^{(k+1)}_i \leftarrow \frac{b_i - \sigma}{a_{ii}}$\\
}
\caption{Pseudo-code for the worker}
\label{alg:paralgo}
\end{algorithm}


As evident from Algorithm~\ref{alg:paralgo} the performance model now also depends on $w$, so the latency, $T_{jacobi}(n, w)$, can be expanded --- ignoring $T_{conv}(n)$ and $T_{upd}(n)$ --- as:
\[
	 T_{jacobi}(n, w) \approx k \cdot ( T_{setup} (n, w) + T_{barrier} (n, w) +  \frac{T_{comp}(n)}{w})
\]
where $T_{setup} (n, w)$ is the time required to setup the $w$ workers and assign them the work (i.e.\ the time needed to the emitter), $T_{barrier} (n, w)$ is the total time spent by the collector and $\frac{T_{comp}(n)}{w}$ is the ideal time needed to $w$ workers to compute the new value of the approximation of the solution.