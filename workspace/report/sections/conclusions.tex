Summing up, we can conclude that:
\begin{itemize}
%	\item The time spent in setting up the workers and in the barrier is not always negligible, especially as $N$ increases
	\item The ratio between the time spent in setup/barrier and the latency increases as $w$ increases, especially for smaller values of $N$.
	\item Even if the experiments seem to imply that the algorithm performs better for bigger values of $N$, huge systems may not perform well since cache coherence procedures may spend non negligible time in moving data (i.e. incrementing the time for updating).
	\item Moreover small workloads (i.e.\ small values of $N/w$) in combination with \emph{vectorization} --- which dramatically improves the computation time --- do not play well together since the the setup/barrier time fraction increases.
\end{itemize}

So, decent performances as Table~\ref{tab:res0} and Table~\ref{tab:res1} were achieved.
To have a better programs in term of measured metrics one could, for example:
\begin{itemize}
	\item Optimize the code to work even better on a specific architecture,
	\item removing unnecessary memory allocations (e.g.\ by re-using workers) or use better allocators,
	\item reduce idle time of workers by giving a more evenly distributed workload.
\end{itemize} 